#### 크롤링이란 간단하게 말하면 인터넷에 있는 웹페이지를 방문해서 그곳에 있는 정보를 수집하는 것이다.
- 이런 작업을 자동으로 하는 프로그램을 `'크롤러'` 또는 `'스파이더'` 라고 부르는데, 이름처럼 웹을 돌아다니며 정보를 수집한다고 생각하면 이해하기 쉽다.
- 데이터 과학에서 크롤링은 매우 중요한 단계다. 왜냐하면, 데이터를 필요로 하는데 그 데이터를 얻는 가장 풍부한 곳이 바로 인터넷이기 때문이다.
- 인터넷은 방대한 정보의 바다라고 할 수 있고, 여기서 필요한 정보를 수집해서 데이터 분석에 활용하면 좋다.

Python에서는 Beautiful Soup라는 라이브러리를 이용해서 웹 크롤링을 진행할 수 있다. 한번 웹페이지의 HTML 정보를 가져와 보는 간단한 코드를 살펴보자.
```python
from bs4 import BeautifulSoup
import requests

url = 'https://www.example.com'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

print(soup.prettify())
```
1. 이 코드에서 requests 라이브러리를 이용해 웹페이지에 요청을 보내고
2. BeautifulSoup 객체를 만들어 HTML을 파싱한다.
3. 마지막으로 prettify() 함수를 이용해 파싱된 HTML을 보기 좋게 출력한다.

- 크롤링을 통해 웹페이지의 정보를 가져온 후에는, 이 정보를 원하는 방식으로 활용하면 된다.
- 예를 들어, 텍스트 정보를 분석해서 특정 키워드가 얼마나 많이 나타나는지 확인하거나, 이미지나 동영상 링크를 수집하는 등의 작업을 할 수 있다.

#### 크롤링시 주의사항
- 크롤링을 할 때 주의해야 할 점이 있다. 모든 웹페이지가 크롤링을 허용하는 것은 아니다. 따라서 웹페이지마다의 'robots.txt' 파일을 확인해서 크롤링이 허용되는 페이지인지 반드시 확인하고 진행해야 한다
- 웹 크롤링은 처음에는 어려워 보일 수 있지만, 이해하고 나면 인터넷의 정보를 자유롭게 활용할 수 있는 도구가 될 것이다.
- 이런 능력을 갖추고 싶다면, BeautifulSoup 라이브러리를 좀 더 깊게 공부해 보는 것이 좋다. 이 라이브러리를 이용하면 웹페이지의 정보를 더 쉽고, 더 효과적으로 수집할 수 있다는 것을 알게 될 것이다.
---
#크롤링 #web [[파싱이란]] 
