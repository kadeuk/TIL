{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmmBC_ZdplqB"
   },
   "source": [
    "## 2. 크롤링(crawling) 이해 및 기본\n",
    "\n",
    "### 2.1. 크롤링(crawling) 이란?\n",
    "  - Web상에 존재하는 Contents를 수집하는 작업 (프로그래밍으로 자동화 가능)\n",
    "      1. HTML 페이지를 **가져와서**, HTML/CSS등을 **파싱**하고, 필요한 데이터만 추출하는 기법\n",
    "      2. **Open API(Rest API)**를 제공하는 서비스에 Open API를 호출해서, 받은 데이터 중 필요한 데이터만 추출하는 기법\n",
    "      3. **Selenium**등 브라우저를 프로그래밍으로 조작해서, 필요한 데이터만 추출하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctw7vGwvplqD"
   },
   "source": [
    "### 2.2. BeautifulSoup 라이브러리를 활용한 초간단 예제\n",
    "  - HTML의 태그를 파싱해서 필요한 데이터만 추출하는 함수를 제공하는 라이브러리\n",
    "  - [BeautifulSoup 라이브러리 페이지](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "  - 설치 방법\n",
    "    - pip install bs4\n",
    "  - [참고: BeautifulSoup 4 API Guide](http://omz-software.com/pythonista/docs/ios/beautifulsoup_guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VTFWsbBplqD",
    "outputId": "5a439bd0-51a3-4221-fe20-773a00588855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토 | Daum 뉴스\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1) reqeusts 라이브러리를 활용한 HTML 페이지 요청\n",
    "# 1-1) res 객체에 HTML 데이터가 저장되고, res.content로 데이터를 추출할 수 있음\n",
    "res = requests.get('http://v.media.daum.net/v/20170615203441266')\n",
    "\n",
    "# print(res.content)\n",
    "# 2) HTML 페이지 파싱 BeautifulSoup(HTML데이터, 파싱방법)\n",
    "# 2-1) BeautifulSoup 파싱방법\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "# 3) 필요한 데이터 검색\n",
    "title = soup.find('title')\n",
    "\n",
    "# 4) 필요한 데이터 추출\n",
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0epLp2UplqE"
   },
   "source": [
    "### 2.3. BeautifulSoup 라이브러리 활용 다양한 예제\n",
    "  - find() 와 find_all() 메서드 사용법 이해하기\n",
    "  - find() : 가장 먼저 검색되는 태그 반환\n",
    "  - find_all() : 전체 태그 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaakFAYQplqE",
    "outputId": "35bd6fb4-0ae1-41e1-ea92-5d5d7a04cf37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
      "[1]크롤링이란?\n",
      "[1]크롤링이란?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "html = \"<html> \\\n",
    "            <body> \\\n",
    "                <h1 id='title'>[1]크롤링이란?</h1> \\\n",
    "                <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p> \\\n",
    "                <p id='body' align='center'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p> \\\n",
    "            </body> \\\n",
    "        </html>\"\n",
    "\n",
    "# 태그로 검색 방법\n",
    "title_data = soup.find('h1')\n",
    "\n",
    "print(title_data)\n",
    "print(title_data.string)\n",
    "print(title_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2Uy_xHWplqF",
    "outputId": "b996284c-1446-4b67-9e5e-315b694e2b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 가장 먼저 검색되는 태그를 반환\n",
    "paragraph_data = soup.find('p')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_mX1JO6plqF",
    "outputId": "16dc531c-debb-47ea-85d8-8b67cc898155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
      "[1]크롤링이란?\n",
      "[1]크롤링이란?\n"
     ]
    }
   ],
   "source": [
    "# 태그에 있는 id로 검색 (javascript 예를 상기!)\n",
    "title_data = soup.find(id='title')\n",
    "\n",
    "print(title_data)\n",
    "print(title_data.string)\n",
    "print(title_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ca4rUByplqF",
    "outputId": "17119d6b-2184-4e00-a973-7eee3c742318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법1\n",
    "paragraph_data = soup.find('p', class_='cssstyle')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3T1l5P1plqG",
    "outputId": "bbaf2258-dc4a-45ad-d5c0-e45e12f3afd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법2\n",
    "paragraph_data = soup.find('p', 'cssstyle')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVf-l_6XplqG",
    "outputId": "e18f6578-04af-4bcc-a16e-a9abf420067c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p align=\"center\" id=\"body\">파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
     ]
    }
   ],
   "source": [
    "# HTML 태그와 태그에 있는 속성:속성값을 활용해서 필요한 데이터를 추출하는 방법\n",
    "paragraph_data = soup.find('p', attrs = {'align': 'center'})\n",
    "print(paragraph_data)\n",
    "print(paragraph_data.string)\n",
    "print(paragraph_data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmP8gzW7plqG",
    "outputId": "d75a550a-646b-40c3-9269-82dd786def10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>웹페이지에서 필요한 데이터를 추출하는 것</p>, <p id=\"body\">파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>]\n",
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
     ]
    }
   ],
   "source": [
    "# find_all() 관련된 모든 데이터를 리스트 형태로 추출하는 함수\n",
    "paragraph_data = soup.find_all('p')\n",
    "\n",
    "print(paragraph_data)\n",
    "print(paragraph_data[0].get_text())\n",
    "print(paragraph_data[1].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxH6_oL4plqG"
   },
   "source": [
    "### 2.4. BeautifulSoup 라이브러리 활용 string 검색 예제\n",
    " - 태그가 아닌 문자열 자체로 검색\n",
    " - 문자열, 정규표현식 등등으로 검색 가능\n",
    "   - 문자열 검색의 경우 한 태그내의 문자열과 exact matching인 것만 추출\n",
    "   - 이것이 의도한 경우가 아니라면 정규표현식 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxtLks3IplqG",
    "outputId": "831757de-aa0d-4b15-97df-23338a992b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오대석']\n",
      "['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']\n",
      "[]\n",
      "[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI | Daum 뉴스\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('http://v.media.daum.net/v/20170518153405933')\n",
    "soup = BeautifulSoup(res.content, 'html5lib')\n",
    "\n",
    "print (soup.find_all(string='오대석'))\n",
    "print (soup.find_all(string=['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']))\n",
    "print (soup.find_all(string='AI'))\n",
    "print (soup.find_all(string=re.compile('AI'))[0])\n",
    "# print (soup.find_all(string=re.compile('AI')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "VPjCdngvplqH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHi1_yFLplqH"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font color=\"blue\" size=\"4em\">연습문제1</font><br>\n",
    " 1. 다음 사이트에서 링크가 되어 있는 모든 제목을 가져와서 출력합니다.<br>\n",
    "    http://media.daum.net/digital/ <br>\n",
    " <br>\n",
    "    X 참고코드: git 저장소에서 02_examples/crawling_seeko_title.py 를 참고 <br>\n",
    "    - 프로그래밍은 스스로 작성을 해야 합니다. 정 이해하기 어려울 때만 참고코드를 보시면 좋을 것 같습니다.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "7uomg-PDplqH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2TR0GaecplqH",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "105673ad-a6f9-4129-d398-fdac4e8462f3"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-d1735dd44598>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-d1735dd44598>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    [------------------------------------------------------]\u001b[0m\n\u001b[1;37m                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "res = requests.get('http://media.daum.net/digital/')\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "# find_all() 메서드를 사용해서 태그와 클래스이름으로 링크가 걸려있는 기사 타이틀을 가져오기\n",
    "[------------------------------------------------------]\n",
    "\n",
    "for num in range(len(link_title)):\n",
    "    print(link_title[num].get_text().strip())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
