{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLeIMTBlzbfw"
      },
      "source": [
        "## 2. 크롤링(crawling) 이해 및 기본\n",
        "\n",
        "### 2.1. 크롤링(crawling) 이란?\n",
        "  - Web상에 존재하는 Contents를 수집하는 작업 (프로그래밍으로 자동화 가능)\n",
        "      1. HTML 페이지를 **가져와서**, HTML/CSS등을 **파싱**하고, 필요한 데이터만 추출하는 기법\n",
        "      2. **Open API(Rest API)**를 제공하는 서비스에 Open API를 호출해서, 받은 데이터 중 필요한 데이터만 추출하는 기법\n",
        "      3. **Selenium**등 브라우저를 프로그래밍으로 조작해서, 필요한 데이터만 추출하는 기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9HTg49dzbfy"
      },
      "source": [
        "### 2.2. BeautifulSoup 라이브러리를 활용한 초간단 예제\n",
        "  - HTML의 태그를 파싱해서 필요한 데이터만 추출하는 함수를 제공하는 라이브러리\n",
        "  - [BeautifulSoup 라이브러리 페이지](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "  - 설치 방법\n",
        "    - pip install bs4\n",
        "  - [참고: BeautifulSoup 4 API Guide](http://omz-software.com/pythonista/docs/ios/beautifulsoup_guide.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia9TVbhhzbfy",
        "outputId": "1ae1dfe8-7fb0-4580-bc9a-ca9579fc52b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "잔금대출에도 DTI 규제 적용 검토 | Daum 뉴스\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 1) reqeusts 라이브러리를 활용한 HTML 페이지 요청\n",
        "# 1-1) res 객체에 HTML 데이터가 저장되고, res.content로 데이터를 추출할 수 있음\n",
        "res = requests.get('http://v.media.daum.net/v/20170615203441266')\n",
        "\n",
        "# print(res.content)\n",
        "# 2) HTML 페이지 파싱 BeautifulSoup(HTML데이터, 파싱방법)\n",
        "# 2-1) BeautifulSoup 파싱방법\n",
        "soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "# 3) 필요한 데이터 검색\n",
        "title = soup.find('title')\n",
        "\n",
        "# 4) 필요한 데이터 추출\n",
        "print(title.get_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqv8qIubzbfz"
      },
      "source": [
        "### 2.3. BeautifulSoup 라이브러리 활용 다양한 예제\n",
        "  - find() 와 find_all() 메서드 사용법 이해하기\n",
        "  - find() : 가장 먼저 검색되는 태그 반환\n",
        "  - find_all() : 전체 태그 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVa-6JLHzbf0",
        "outputId": "d97c72b3-b58e-49d9-cc14-b53776491029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
            "[1]크롤링이란?\n",
            "[1]크롤링이란?\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "html = \"<html> \\\n",
        "            <body> \\\n",
        "                <h1 id='title'>[1]크롤링이란?</h1> \\\n",
        "                <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p> \\\n",
        "                <p id='body' align='center'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p> \\\n",
        "            </body> \\\n",
        "        </html>\"\n",
        "\n",
        "# 태그로 검색 방법\n",
        "title_data = soup.find('h1')\n",
        "\n",
        "print(title_data)\n",
        "print(title_data.string)\n",
        "print(title_data.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1hAEVUBzbf0",
        "outputId": "e4050375-bcf7-4399-d97d-2475cc8484b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
            "웹페이지에서 필요한 데이터를 추출하는 것\n",
            "웹페이지에서 필요한 데이터를 추출하는 것\n"
          ]
        }
      ],
      "source": [
        "# 가장 먼저 검색되는 태그를 반환\n",
        "paragraph_data = soup.find('p')\n",
        "\n",
        "print(paragraph_data)\n",
        "print(paragraph_data.string)\n",
        "print(paragraph_data.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2YgGd6zzbf0",
        "outputId": "a51105eb-f640-4bca-fa8f-602c3fc75858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
            "[1]크롤링이란?\n",
            "[1]크롤링이란?\n"
          ]
        }
      ],
      "source": [
        "# 태그에 있는 id로 검색 (javascript 예를 상기!)\n",
        "title_data = soup.find(id='title')\n",
        "\n",
        "print(title_data)\n",
        "print(title_data.string)\n",
        "print(title_data.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNVeHI7bzbf0",
        "outputId": "594ef720-9775-484c-bf79-b613a1448f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
            "웹페이지에서 필요한 데이터를 추출하는 것\n",
            "웹페이지에서 필요한 데이터를 추출하는 것\n"
          ]
        }
      ],
      "source": [
        "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법1\n",
        "paragraph_data = soup.find('p', class_='cssstyle')\n",
        "\n",
        "print(paragraph_data)\n",
        "print(paragraph_data.string)\n",
        "print(paragraph_data.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdgY6CgJzbf1",
        "outputId": "8dc60a62-f37f-4069-94e6-80f01449f1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<p class=\"cssstyle\">웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
            "웹페이지에서 필요한 데이터를 추출하는 것\n",
            "웹페이지에서 필요한 데이터를 추출하는 것\n"
          ]
        }
      ],
      "source": [
        "# HTML 태그와 CSS class를 활용해서 필요한 데이터를 추출하는 방법2\n",
        "paragraph_data = soup.find('p', 'cssstyle')\n",
        "\n",
        "print(paragraph_data)\n",
        "print(paragraph_data.string)\n",
        "print(paragraph_data.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2VrM9opzbf1",
        "outputId": "ce951578-692f-4f97-8dfb-13859355361e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<p align=\"center\" id=\"body\">파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>\n",
            "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n",
            "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
          ]
        }
      ],
      "source": [
        "# HTML 태그와 태그에 있는 속성:속성값을 활용해서 필요한 데이터를 추출하는 방법\n",
        "paragraph_data = soup.find('p', attrs = {'align': 'center'})\n",
        "print(paragraph_data)\n",
        "print(paragraph_data.string)\n",
        "print(paragraph_data.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk16eLmIzbf1",
        "outputId": "9bca5ec5-cfb9-4f1a-b6b7-15e53c5df1c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<p>웹페이지에서 필요한 데이터를 추출하는 것</p>, <p id=\"body\">파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>]\n",
            "웹페이지에서 필요한 데이터를 추출하는 것\n",
            "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
          ]
        }
      ],
      "source": [
        "# find_all() 관련된 모든 데이터를 리스트 형태로 추출하는 함수\n",
        "paragraph_data = soup.find_all('p')\n",
        "\n",
        "print(paragraph_data)\n",
        "print(paragraph_data[0].get_text())\n",
        "print(paragraph_data[1].get_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snoy84OOzbf1"
      },
      "source": [
        "### 2.4. BeautifulSoup 라이브러리 활용 string 검색 예제\n",
        " - 태그가 아닌 문자열 자체로 검색\n",
        " - 문자열, 정규표현식 등등으로 검색 가능\n",
        "   - 문자열 검색의 경우 한 태그내의 문자열과 exact matching인 것만 추출\n",
        "   - 이것이 의도한 경우가 아니라면 정규표현식 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFm2rsn4zbf2",
        "outputId": "3598376f-61dc-403d-a990-348ee329c0f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['오대석']\n",
            "['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']\n",
            "[]\n",
            "[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI | Daum 뉴스\n"
          ]
        }
      ],
      "source": [
        "res = requests.get('http://v.media.daum.net/v/20170518153405933')\n",
        "soup = BeautifulSoup(res.content, 'html5lib')\n",
        "\n",
        "print (soup.find_all(string='오대석'))\n",
        "print (soup.find_all(string=['[이주의해시태그-#네이버-클로바]쑥쑥 크는 네이버 AI', '오대석']))\n",
        "print (soup.find_all(string='AI'))\n",
        "print (soup.find_all(string=re.compile('AI'))[0])\n",
        "# print (soup.find_all(string=re.compile('AI')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w1dsZF-bzbf2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtcUzN06zbf2"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<font color=\"blue\" size=\"4em\">연습문제1</font><br>\n",
        " 1. 다음 사이트에서 링크가 되어 있는 모든 제목을 가져와서 출력합니다.<br>\n",
        "    http://media.daum.net/digital/ <br>\n",
        " <br>\n",
        "    X 참고코드: git 저장소에서 02_examples/crawling_seeko_title.py 를 참고 <br>\n",
        "    - 프로그래밍은 스스로 작성을 해야 합니다. 정 이해하기 어려울 때만 참고코드를 보시면 좋을 것 같습니다.<br>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dim-syG-zbf2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pXTO3KnLzbf2",
        "outputId": "4830a3e9-aac3-453a-d4a5-6cb00a1be8cf"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-9-d1735dd44598>, line 9)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-d1735dd44598>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    [------------------------------------------------------]\u001b[0m\n\u001b[1;37m                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "res = requests.get('http://media.daum.net/digital/')\n",
        "soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "# find_all() 메서드를 사용해서 태그와 클래스이름으로 링크가 걸려있는 기사 타이틀을 가져오기\n",
        "[------------------------------------------------------]\n",
        "\n",
        "for num in range(len(link_title)):\n",
        "    print(link_title[num].get_text().strip())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}